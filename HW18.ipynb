{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6080a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, dot, Reshape , Input\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9bcd8fb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/r8h0fl6158l8g2_8l65x2zv00000gn/T/ipykernel_5127/2051259223.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(\"corpus_100k\", sep='։\\n', names = ['sentence','-'])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"corpus_100k\", sep='։\\n', names = ['sentence','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18b299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['sentence'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac7441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skip-gram training samples with negative sampling\n",
    "def generate_skipgrams(sentences, vocab_size, window_size=4, negative_samples=1.0, seed=42):\n",
    "    couples, labels = skipgrams(\n",
    "        sentences,\n",
    "        vocabulary_size=vocab_size,\n",
    "        window_size=window_size,\n",
    "        negative_samples=negative_samples,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    word_target, word_context = zip(*couples)\n",
    "    word_target = np.array(word_target, dtype=\"int32\").reshape(-1, 1)\n",
    "    word_context = np.array(word_context, dtype=\"int32\").reshape(-1, 1)\n",
    "    labels = np.array(labels, dtype=\"int32\")\n",
    "    return word_target, word_context, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "negative_samples = 5  \n",
    "\n",
    "word_target, word_context, labels = generate_skipgrams(\n",
    "    tokenizer.texts_to_sequences(data['sentence']),\n",
    "    vocab_size,\n",
    "    window_size=window_size,\n",
    "    negative_samples=negative_samples\n",
    ")\n",
    "\n",
    "#  the word2vec \n",
    "embedding_size = 100 \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=1))\n",
    "model.add(Reshape((embedding_size,)))\n",
    "model.add(Dense(embedding_size, activation='sigmoid'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss=negative_sampling_loss, optimizer='adam')\n",
    "\n",
    "model.fit([word_target, word_context], labels, epochs=10, batch_size=64)\n",
    "\n",
    "embedding_layer_weights = model.layers[0].get_weights()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bdcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE to reduce the dimensionality of the embeddings to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embedding_layer_weights)\n",
    "\n",
    "# Visualize \n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(vocabulary_size):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1])\n",
    "    plt.annotate(word_index_mapping[i], xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                 xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714bb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588cff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088eeb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
